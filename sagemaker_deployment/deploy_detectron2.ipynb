{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting Detectron2 model on Sagemaker Inference endpoint\n",
    "\n",
    "In this notebook we'll package previously trained model into PyTorch Serving container and deploy it on Sagemaker. First, let's review serving container. There are two key difference comparing to training container:\n",
    "- we are using different base container provided by Sagemaker;\n",
    "- we need to start Web server (refer to ENTRYPOINT command)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Serving Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Build an image of Detectron2 with Sagemaker Multi Model Server: https://github.com/awslabs/multi-model-server\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# using Sagemaker PyTorch container as base image\u001b[39;49;00m\r\n",
      "\u001b[37m# from https://github.com/aws/sagemaker-pytorch-serving-container/\u001b[39;49;00m\r\n",
      "\u001b[34mARG\u001b[39;49;00m \u001b[31mREGION\u001b[39;49;00m=us-east-1\r\n",
      "\r\n",
      "\u001b[37m#FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\u001b[39;49;00m\r\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.${REGION}.amazonaws.com/pytorch-inference:1.5.1-gpu-py36-cu101-ubuntu16.04\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m############# Installing latest builds ############\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install --upgrade --force-reinstall torch torchvision cython\r\n",
      "\r\n",
      "\u001b[37m############# D2 section ##############\u001b[39;49;00m\r\n",
      "\u001b[37m# installing dependencies for D2 https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/facebookresearch/fvcore'\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#ENV FORCE_CUDA=\"1\"\u001b[39;49;00m\r\n",
      "\u001b[37m# Build D2 only for Turing architecture - G4 instance family\u001b[39;49;00m\r\n",
      "\u001b[37m#ENV TORCH_CUDA_ARCH_LIST=\"Turing\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Build D2 from latest sources\u001b[39;49;00m\r\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/facebookresearch/detectron2.git'\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Set a fixed model cache directory. Detectron2 requirement\u001b[39;49;00m\r\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFVCORE_CACHE\u001b[39;49;00m=\u001b[33m\"/tmp\"\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize -l docker docker/Dockerfile.serving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in case of training image, we'll need to build and push container to AWS ECR. Before this, we'll need to loging to shared Sagemaker ECR and your local ECR\n",
    "- NOTE: change private ECR address to the one from your AWS ECR instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# loging to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 578480262707.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_and_push.sh  d2_byoc_coco2017_inference.ipynb  detectron2_pred.py  docker\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 777 docker/Dockerfile.serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build and push container using follow command. Note, that here we supply non-default Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting credentials\n",
      "done getting credentials\n",
      "Working in region us-east-1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "logged in\n",
      "dockerfile provided\n",
      "Sending build context to Docker daemon  2.932MB\n",
      "Step 1/7 : ARG REGION=us-east-1\n",
      "Step 2/7 : FROM 763104351884.dkr.ecr.${REGION}.amazonaws.com/pytorch-inference:1.5.1-gpu-py36-cu101-ubuntu16.04\n",
      " ---> a7f350a05bd4\n",
      "Step 3/7 : RUN pip install --upgrade --force-reinstall torch torchvision cython\n",
      " ---> Using cache\n",
      " ---> 7df5de28f4bf\n",
      "Step 4/7 : RUN pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
      " ---> Using cache\n",
      " ---> 0ba02d83cdff\n",
      "Step 5/7 : RUN pip install 'git+https://github.com/facebookresearch/fvcore'\n",
      " ---> Using cache\n",
      " ---> db97f66ddd37\n",
      "Step 6/7 : RUN pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
      " ---> Running in 8c655f9858b7\n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /home/model-server/tmp/pip-req-build-bm_0eav4\n",
      "\u001b[91m  Running command git clone -q https://github.com/facebookresearch/detectron2.git /home/model-server/tmp/pip-req-build-bm_0eav4\n",
      "\u001b[0m  Resolved https://github.com/facebookresearch/detectron2.git to commit 210809711838d5bd193e8bb3bc4bab39a660bf5b\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (3.3.4)\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (1.1.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (0.8.10)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (4.62.3)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (0.1.5)\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting omegaconf>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.1-py3-none-any.whl (154 kB)\n",
      "Collecting black\n",
      "  Downloading black-22.8.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Collecting timm\n",
      "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from detectron2==0.6) (0.8)\n",
      "\u001b[91mERROR: Package 'detectron2' requires a different Python: 3.6.13 not in '>=3.7'\n",
      "\u001b[0mThe command '/bin/sh -c pip install 'git+https://github.com/facebookresearch/detectron2.git'' returned a non-zero code: 1\n",
      "done building dockerfile\n",
      "d2-sm-coco-serving\n",
      "The push refers to repository [578480262707.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco-serving]\n",
      "\n",
      "\u001b[1B305abdee: Preparing \n",
      "\u001b[1Bb70b6a37: Preparing \n",
      "\u001b[1Bc6241eb0: Preparing \n",
      "\u001b[1B2f260b61: Preparing \n",
      "\u001b[1Bd75e994c: Preparing \n",
      "\u001b[1B98cc36db: Preparing \n",
      "\u001b[1B1c0b4a34: Preparing \n",
      "\u001b[1Bc0b99e87: Preparing \n",
      "\u001b[1B031a99d6: Preparing \n",
      "\u001b[1B5b5e6cca: Preparing \n",
      "\u001b[1B352dc1d4: Preparing \n",
      "\u001b[1Bfff8f3ad: Preparing \n",
      "\u001b[1Bfead2ef7: Preparing \n",
      "\u001b[1Bca0b56b9: Preparing \n",
      "\u001b[1B85ccdb8b: Preparing \n",
      "\u001b[1B2143251e: Preparing \n",
      "\u001b[1B78092553: Preparing \n",
      "\u001b[1Bd93f5aa0: Preparing \n",
      "\u001b[1B34d55dcb: Preparing \n",
      "\u001b[1B23ca7bba: Preparing \n",
      "\u001b[1B9a96e575: Preparing \n",
      "\u001b[1B6cf7f14c: Preparing \n",
      "\u001b[1Bf776d2d7: Preparing \n",
      "\u001b[1B6e3b01ec: Preparing \n",
      "\u001b[1B23951136: Preparing \n",
      "\u001b[1B9b94257f: Preparing \n",
      "\u001b[1Ba5c0b62c: Preparing \n",
      "\u001b[1Bccffb13d: Preparing \n",
      "\u001b[1Be1ca342e: Preparing \n",
      "\u001b[1B3a1d32ab: Preparing \n",
      "\u001b[1Be0e6285f: Preparing \n",
      "\u001b[1B0d4c1cc0: Preparing \n",
      "\u001b[1B1253de72: Preparing \n",
      "\u001b[1B7b6ba1ab: Preparing \n",
      "\u001b[1B9e937b48: Preparing \n",
      "\u001b[1B125d08e2: Preparing \n",
      "\u001b[1B2d5b2da2: Layer already exists \u001b[32A\u001b[2K\u001b[27A\u001b[2K\u001b[21A\u001b[2K\u001b[16A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:c6c8c50793126d02b0efd38c65ee891a001599a352f1cdd389042d776bfa6248 size: 8087\n"
     ]
    }
   ],
   "source": [
    "!bash build_and_push.sh d2-sm-coco-serving latest docker/Dockerfile.serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Inference Endpoint\n",
    "\n",
    "Below is some initial imports and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlogger\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m setup_logger\r\n",
      "setup_logger() \u001b[37m# this logs Detectron2 information such as what the model is doing when it's training\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# import some common libraries\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcv2\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# import some common detectron2 utilities\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m model_zoo\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mengine\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DefaultPredictor \u001b[37m# a default predictor class to make predictions on an image using a trained model\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mconfig\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_cfg \u001b[37m# a config of \"cfg\" in Detectron2 is a series of instructions for building a model\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mvisualizer\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Visualizer \u001b[37m# a class to help visualize Detectron2 predictions on an image\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdetectron2\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MetadataCatalog \u001b[37m# stores information about the model such as what the training/test data is, what the class names are\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Target classes with spaces removed\u001b[39;49;00m\r\n",
      "target_classes = [\u001b[33m'\u001b[39;49;00m\u001b[33mBathtub\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mBed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mBilliard table\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mCeiling fan\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mCoffeemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mCouch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mCountertop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mDishwasher\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mFireplace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mFountain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mGas stove\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mJacuzzi\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mKitchen & dining room table\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mMicrowave oven\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mMirror\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mOven\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mPillow\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mPorch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mRefrigerator\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mShower\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mSink\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mSofa bed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mStairs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mSwimming pool\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mTelevision\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mToilet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mTowel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mTree house\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mWashing machine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                  \u001b[33m'\u001b[39;49;00m\u001b[33mWine rack\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "idx2class = {i:\u001b[36mcls\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m i, \u001b[36mcls\u001b[39;49;00m \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(target_classes)}\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_predictor\u001b[39;49;00m(config_path, model_path):\r\n",
      "    cfg = get_cfg()\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mgot config\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    cfg.merge_from_file(config_path) \u001b[37m# get baseline parameters from YAML config\u001b[39;49;00m\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mmerged\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    cfg.MODEL.WEIGHTS = model_path\r\n",
      "    cfg.MODEL.DEVICE=\u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        pred = DefaultPredictor(cfg)\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mcreated pred\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        pred.model.eval()\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33meval mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel loading failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.error(e) \r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m pred\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    Deserialize and load D2 model. This method is called automatically by Sagemaker.\u001b[39;49;00m\r\n",
      "\u001b[33m    model_dir is location where your trained model will be downloaded.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[37m# Restoring trained model, take a first .yaml and .pth/.pkl file in the model directory\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m os.listdir(model_dir):\r\n",
      "            \u001b[37m# looks up for yaml file with model config\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m file.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\r\n",
      "                config_path = os.path.join(model_dir, file)\r\n",
      "            \u001b[37m# looks up for *.pkl or *.pth files with model weights\u001b[39;49;00m\r\n",
      "            \u001b[34mif\u001b[39;49;00m file.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[35mor\u001b[39;49;00m file.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.pkl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\r\n",
      "                model_path = os.path.join(model_dir, file)\r\n",
      "        \r\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing config file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mconfig_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing model weights from \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "        \r\n",
      "        pred = _get_predictor(config_path,model_path)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m pred\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel deserialization failed...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.error(e)  \r\n",
      "        \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDeserialization completed ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    Converts image from NPY format to numpy.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mHandling inputs...Content type is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mrequest_content_type\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mjpeg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m request_content_type:\r\n",
      "            nparr = np.frombuffer(request_body, np.uint8)\r\n",
      "            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\r\n",
      "            input_object = np.asarray(img)\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m input_object\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported request content type \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mrequest_content_type\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mInput deserialization failed...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.error(e)  \r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "            \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mInput deserialization completed...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInput object type is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mtype\u001b[39;49;00m(input_object)\u001b[33m}\u001b[39;49;00m\u001b[33m and shape \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_object.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_object, model):\r\n",
      "    \u001b[37m# according to D2 rquirements: https://detectron2.readthedocs.io/tutorials/models.html\u001b[39;49;00m\r\n",
      "    \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDoing predictions...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.debug(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInput object type is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mtype\u001b[39;49;00m(input_object)\u001b[33m}\u001b[39;49;00m\u001b[33m and shape \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_object.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.debug(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mPredictor type is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mtype\u001b[39;49;00m(model)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        prediction = model(input_object)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m prediction\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mPrediction failed...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.error(e)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mPredictions are:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.debug(prediction)\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, response_content_type):\r\n",
      "    \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcessing output predictions...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    logger.debug(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mOutput object type is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mtype\u001b[39;49;00m(prediction)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        pred_classes = []\r\n",
      "        preds = prediction[\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to(\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mput into cpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(preds)):\r\n",
      "            pred_class = preds[i].pred_classes.item()\r\n",
      "            logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mpred.item\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "            score = preds[i].scores.item()\r\n",
      "            logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mscore.item\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "            \u001b[34mif\u001b[39;49;00m score >= \u001b[34m0.10\u001b[39;49;00m:\r\n",
      "                pred_classes.append(pred_class)\r\n",
      "        \r\n",
      "        pred_classes = \u001b[36mlist\u001b[39;49;00m(\u001b[36mset\u001b[39;49;00m(pred_classes))\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mlist(set(pred))\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        pred_classes = [idx2class[i] \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m pred_classes]\r\n",
      "        logger.info(pred_classes)\r\n",
      "        output = json.dumps(pred_classes)\r\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mpred_classes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m output\r\n",
      "        \r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "        logger.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mOutput processing failed...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        logger.error(e)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n",
      "    \r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mOutput processing completed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize detectron2_pred.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = \"us-east-1\"\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "prefix_input = 'detectron2-input'\n",
    "prefix_output = 'detectron2-ouput'\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters of your container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following container will be used for hosting:  578480262707.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco-serving:latest\n"
     ]
    }
   ],
   "source": [
    "container_serving = \"d2-sm-coco-serving\" # your container name\n",
    "tag = \"latest\" # you can have several version of container available\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, container_serving, tag)\n",
    "\n",
    "print(\"Following container will be used for hosting: \", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy remote endpoint\n",
    "\n",
    "To process inference data when we are sending it over internet, we need to have two customer ser/deser methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2844\r\n",
      "   4 -rw-rw-r-- 1 ec2-user ec2-user    2296 Dec 21 01:58 build_and_push.sh\r\n",
      "  40 -rw-rw-r-- 1 ec2-user ec2-user   37659 Dec 21 07:30 d2_byoc_coco2017_inference.ipynb\r\n",
      "2788 -rw-rw-r-- 1 ec2-user ec2-user 2851732 Dec 21 06:42 demo.jpeg\r\n",
      "   8 -rw-rw-r-- 1 ec2-user ec2-user    6157 Dec 21 07:31 detectron2_pred.py\r\n",
      "   4 drwxrwxr-x 2 ec2-user ec2-user    4096 Dec 21 06:26 \u001b[0m\u001b[01;34mdocker\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel, PyTorch, PyTorchPredictor\n",
    "from sagemaker.estimator import Estimator, Model\n",
    "import boto3\n",
    "\n",
    "remote_model = PyTorchModel(name = \"d2-service-v3\", \n",
    "                             model_data=\"s3://cc-finalproj-amenity-model/model.tar.gz\", # s3 path that stores your detectron model training output\n",
    "                             role=role,\n",
    "                             sagemaker_session = sess,\n",
    "                             entry_point=\"detectron2_pred.py\",\n",
    "                             framework_version=\"2\", py_version=\"3\",\n",
    "                             image_uri=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Using already existing model: d2-service-v3\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = f\"{container_serving}-{tag}-inference\"\n",
    "\n",
    "remote_predictor = remote_model.deploy(\n",
    "                         instance_type='ml.m5.4xlarge', \n",
    "                         initial_instance_count=1,\n",
    "                         update_endpoint = True, # comment or False if endpoint doesns't exist\n",
    "                         endpoint_name=endpoint_name, # define a unqie endpoint name; if ommited, Sagemaker will generate it based on used container\n",
    "                         tags=[{\"Key\":\"image\", \"Value\":f\"{container_serving}:{tag}\"}], \n",
    "                         wait=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/mrdbourke/airbnb-amenity-detection/master/custom_images/airbnb-article-cover.jpeg -O demo.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference\n",
    "endpoint_name = f\"{container_serving}-{tag}-inference\"\n",
    "b = \"cc-proj-imagebucket\"\n",
    "k = \"listing1_1671591527027.png\"\n",
    "s3 = boto3.client('s3')\n",
    "res = s3.get_object(\n",
    "    Bucket=b,\n",
    "    Key=k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bytes = res[\"Body\"].read()\n",
    "#image_bytes[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "accept_type = \"json\" # \"json\" or \"detectron2\". Won't impact predictions, just different deserialization pipelines.\n",
    "content_type = 'image/jpeg'\n",
    "endpoint_name =  f\"{container_serving}-{tag}-inference\"\n",
    "payload = image_bytes\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType=content_type,\n",
    "    Accept = accept_type\n",
    ")\n",
    "\n",
    "\n",
    "predictions = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bed', 'Couch', 'Mirror', 'Pillow', 'Shower', 'Sofa bed']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
